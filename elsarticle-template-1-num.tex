%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\journal{Journal Name}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Generative peptide design}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Patrick Breen}

\address{Athens Georgia, United States}

\begin{abstract}
%% Text of abstract
With the developments in the fields of machine learning, bioinformatics and combinatorial chemistry, scientists hope to be able to search exponentially sized combinatorial search spaces to identify drug targets and to aid in synthetic evolution strategies. Because of both the exponential size of the molecular search space and the complex nature of the objective function, efficent startegies for identifying optimal drug targets is difficult. It is possible that we could use an unsupervised method to produce a lower dimensional latent representation that accounts for the distribution of naturally occuring molecules. Ideally this lower dimensional latent distribution also captures "functional similarity" and can be used to iteratively improve a molecular solution under some objective function. In this work we present such a latent representation for peptides and discuss how our model could be used for peptide design.
\end{abstract}

\begin{keyword}
Machine Learning \sep Bioinformatics \sep Computational Biology
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers

%% main text
\section{Introduction}

The concept, though developed independantly back in September 2016 is similar to a paper by Gomez-Bombarelli et al published in October 2016 titled "Automatic chemical design using data-driven continuous representation of molecules"\cite{gomez2016automatic}. The cited paper attempts to create a generative continuous representation of small molecules for drug-related machine learning purposes. My work will propose a generative continous representation of peptides. A Variational Autoencoder (VAE) combined with a Recurrent eural Network (RNN) Sequence to Sequence (seq2seq) model is the specific model we used in our project. The model used by Gomez-Bombarelli et al also used a VAE, but used a more "vanilla" deep neural network instead of a seq2seq network. In addition, the seq2seq network that we used took advantage of some advanced features such as an "attention mechanism" and "bucketing". Our model was implemented in tensorflow.

In this project we take have taken the open uniref dataset, filtered for sequences between 10 and 100 amino acids in length, and subsampled one million peptide sequences (enough to fit into memory). One of the most important hyper parameters in our model is $n_z$, which denotes the size of the latent distribution that "encodes" the peptide. By choosing $n_z$ to be large (32) our model was easily able to encode peptides with nearly $100\%$ accuracy. When $n_z$ was small (2), our model was only able to encode patterns in peptides that are extremely "highly conserved" such as the peptide beginning with a Methionine.

We hope to in the next few weeks demonstrate the utility of our seq2seq VAE peptide model. We will show a variety of figures including the relative encoding distances between the input tokens (tokens are amino acid 2-mers and encoded prior to being input into the RNN). Inspection of the embeddings will show us whether the token XY is similar to the token YX for various amino acids. Embeddings will also let us see relative substitution rates, which are not explicitly fed into the model from something like a Blossum matrix, but rather are "learned organically".

We also want to look at error (both reconstruction error and a latent error which captures the entropy of the latent distribution) and example reconstructions for a range of hyper parameter choices of $n_z$. We want to see if there is any accuracy performance correlation with sequence length.

Finally, we want to examine the latent distribution to see if we can see structures, or clusters that can be interpreted as "biologically meaningful". We can do this by visualizing the latent distribution in 2 dimensions using dimensionality reduction tools like tSNE or PCA. We can use a variety of clustering algorithms such as kmeans or spectral clustering to determine clusters, and or perhaps color data points in latent space by biological function. Having biological function labels would also allow us to do supervised classification, and to use latent space representation to extend this with semisupervised classification. Thus our model would provide a representation that could be shown to be useful for a variety of machine learning problems concerning peptides.

Our model provides more than just a continuous representation of peptides. By controlling the regularization parametter ($n_z$) we can generate a distribution for a given input peptide of peptides that are related to the input peptide. This allows us to do say "guided evolution" where we can draw a mutated peptide from a distribution that models relative substitution rates in the uniref data set. This generated distribution allows us to perform a variety of subsequent machine learning applications on peptides. Say we have an objective function for how well a given peptide performs at binding or for drug treatment. This objective function would be filled with local optima, not differentiable, and expensive to compute (requiring experiment or molecular simulation). We could construct an an optimization strategy of this function more effective than monto carlo or exhastive search, if we use our proposed model as the distribution function in a Metropolis Hastings optimization. Such a proposed application would be similar in theory to previous work, but with a different distribution function\cite{giguere2013improved}.

One large assumption used in this project is the seq2seq error function which serves as the objective function for the whole model. Currently we used a very simple dense sequence comparison (I don't know the details, need to look this up.) but in the future we might want to extend this model by using a seq2seq error function that explicitly accounts for insertions and deletions present in biological sequences. For now, we consider this problem to be mitigated in part by focusing on relatively small peptides (less than 100 amino acids) and we assume that small conserved motifs of amino acids (which is what our model is capturing) are unlikely to contain insertions or deletions. Insertions and deletions are more likely to occur between biologically concerced motifs. Our model currently does allow for insertions and deletions in a general seq2seq way, but it does not handle them explicitly.

\section{Model and Data}

The model was implemented using the Tensorflow deep learning library\cite{tensorflow2015-whitepaper}. The model overall is a sequence to sequence autoencoder, in which a peptide, represented as a sequence, is mapped into a latent distribution which is then sampled to generate a state vector that reproduces the initial peptide. By setting each amino acid equal to a unit (word) in our input and output sequence, we could think of our model as a translation model with vocabulary size 20 (the number of canonical amino acids). Though this simplistic approach would work, we wanted to encorporate advanced translation methods including embedding via negative sampling. This required us to increase our "vocabulary size". Therefore, instead of using individual amino acids as "words" we used amino acid 2-mers, resulting in a vocabulary size of 400 canonical amino acid 2-mers. We replaced non-canonical amino acids with an "unknown" token represented as a '\_' character.

Our data was aquired from the uniref90 dataset, a dataset that contains non-redundant proteins and peptides with length of at least 11 amino acids. From this dataset we constrained our model to peptides of less than 100 amino acids resulting in about 6.1 million peptides. We uniformly sampled 10000 of these peptides as a test set, and put the rest in our training set. Note that our dataset, while large, is much less than the $20^{100}$ total possible possible peptides. By constraining our model to peptides that are found in nature we heavily bias our model to only a small fraction of the combinatorial search space.

Both the training and testing peptides were "bucketed" into 4 buckets based on amino acid 2-mer length, 5-10, 10-20, 20-30 and 30-50. The sequence length distributions are given in Figure \ref{fig:distribution_data_testing} and \ref{fig:distribution_data_training}. Note the scales.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/seq_lengths_test.png}
  \caption{Distribution of amino acid 2-mer sequence lengths in the testing set.}
  \label{fig:distribution_data_testing}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/seq_lengths_train.png}
  \caption{Distribution of amino acid 2-mer sequence lengths in the training set.}
  \label{fig:distribution_data_training}
\end{figure}

The sequence-to-sequence model starts with the encoder which uses embedding and and RNN based on gated reccurent units (GRU) a simpler RNN unit than long sort term memory (LSTM) units, but with comperable performance \cite{chung2014empirical}. The size of the embedding vectors, which was the same as the size of the GRU units, was 64. The reccurent neural network had 2 layers, and used attention\cite{chorowski2014end}. The encoder state was mapped into a vector of length $n_z$ representing the mean of a normal distribution and a vector of length $n_z$ representing the standard deviation of a normal distribution. This disrubution was sampled and decoded using a decoder GRU-based reccurent neural network. This type of generative latent distribution model is known as a variational autoencoder (VAE) and has been used in other molecular representation papers\cite{gomez2016automatic}. See Figure \ref{model_diagram} for a diagram of the model.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/model_diagram.eps}
  \caption{diagram of sequence to sequence peptide model.}
  \label{model_diagram}
\end{figure}

The objective function used to train our model is a sum of the reconstruction loss, the error between the input and output sequences, and the latent loss, a regularization term (both measured in log-perplexity. My minimizing the latent loss, we prevent over fitting our model to the training data. The reconstruction loss was defined as the weighted cross entropy for a sequence, a loss function that has worked well in english to french translation, as well as general sequence to sequence applications\cite{vinyals2015grammar}. However, despite these successes, it remains to be seen if it is the most appropriate loss function for peptide sequences.

\section{Results}

- Testing loss and latent loss with $n_z$ 2, 4, 8

In order to control the complexity model that captures the peptide distribution, we can train our model with a variety of choices of the hyper paramenter $n_z$. To determine the accuracy of the reconstruction, we chart the loss time on the test set. Figure \ref{fig:loss_fig} shows the reconstruction loss (denoted "loss") and the latent loss (denoted "latent loss") for $n_z$ = 2, 4 and 8.

- Example translations with $n_z$ 2, 4, 8

Another way of showing reconstruction loss is by doing example translations. Here we show 5 different original sequences, each followed by 3 example reconstructions. Note that due to the latent distribution being probabalistic, these example reconstructions are non-deterministic. In fact, due to the use of the RNN encoder and decoders, the reconstructed peptides can be of varying lengths. This allows us to model a form of insertion/deletion mutation.

- Examine embeddings say for just $n_z$ 2

Prior to embedding, and after translation, our model uses embedding operations. These operations allow the model to map each 2-mer amino acid token into a dense representation as part of the overall model. We can look at these embeddings and map their relative differences in "embedding space" which captures 2-mer amino acid substitutability.

- Examine "most simmilar peptides in latent space" for just $n_z$ 8

Lets take an example of a peptide and find all peptides close to it in latent space.

- Plot distribution of latent space for just $n_z$ 8

Finally, we can plot the latent means of the distributions for each point in the testing set.

\section{Conclusions}


\bibliographystyle{model1-num-names}
\bibliography{sample.bib}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.