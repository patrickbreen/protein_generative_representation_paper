%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\journal{Journal Name}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Generative peptide embedding}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Patrick Breen}

\address{Athens Georgia, United States}

\begin{abstract}
%% Text of abstract
With the developments in the fields of machine learning, bioinformatics and combinatorial chemistry, scientists hope to be able to search exponentially sized combinatorial search spaces to identify drug targets and aid in synthetic evolution strategies. Because of both the exponential size of the molecular search space and the complex nature of the objective function, efficent startegies for identifying optimal drug targets is difficult. It is possible that we could use an unsupervised method to produce a lower dimensional latent representation that accounts for the distribution of naturally occuring molecules. Ideally this lower dimensional latent distribution also captures "functional similarity" and can be used to iteratively improve a molecular solution under some objective function. In this work we present such a latent representation for peptides and discuss how our model could be used for peptide design.
\end{abstract}

\begin{keyword}
Deep Learning \sep Bioinformatics \sep Generative Modeling
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers

%% main text
\section{Introduction}

The design space of all peptides of length N is combinatorial in N, or approximatley $20^N$. All recorded peptides that occur in the uniprot database is a much smaller number, though still quite large at about 10 million peptides less than 100 residues. Combinatorial chemistry developed in the 1990s suggested that new biologics could be produced by searching the massive search space of both existing and possible peptides for a given binding or therputic property. However, such a brute force search would be costly and take decades so researchers started to develop heuristics and strategies to make combinatorial drug search more efficent\cite{feher2003property}. By taking existing molecules and combining them, they had some success at producing, or "designing" useful molecules in more effient ways than brute force search of the combinatorial search space.

More recently, the widespread adoption of machine learning has allowed us to use deep learning to produce scalable and heirachical models for data representation. Unsupervised models such as principle components analysis (PCA) which is based in simple linear algebra, could be scaled hierarchically and made non-linear with the use of autoencoders\cite{hinton1994autoencoders}. Use of these unsupervised approaches could produce an efficient data representation that could be used to improve supervised learning, i.e. in our application domain finding optimal molecular configurations in a semi-supervised manner. Unsupervised learning could also be used simply to cluster simmilar molecules for research purposes.

A special type of autoencoder, known as a variational autoencoder (VAE), extends the autoencoder model by adding a layer of probabalistic modeling\cite{kingma2013auto}. A VAE allows one to map a given data point to a distribution that if sampled will represent a probability distribution that could recreate the initial data point. This allows one to not only determine a representation, but also to sample synthetic data points (not in the training data) that are simmilar to a given data point. This allows a probabalistic way of exploring "synthetic" data points in the input space, more on this later.

In the application of molecular modeling, a recent paper "Automatic chemical design using data-driven continuous representation of molecules"\cite{gomez2016automatic} used a VAE to produce a generative model of small drug molecules. The cited paper attempts to create a generative continuous representation of small molecules for drug-design related machine learning purposes. The work in this paper will describe a simmilar model, namely a generative continous representation of peptides. In my model I use a VAE combined with a Recurrent eural Network (RNN). The model used by Gomez-Bombarelli et al also used a VAE, but used a more "vanilla" feed forward deep neural network instead of a sequence to sequence network. In addition, the seqeuence to sequence network that we used took advantage of some advanced features such as an "attention mechanism" and "bucketing" (see Model and Data section for more description).

\section{Model and Data}

The generative peptide representation model was implemented using the Tensorflow deep learning library\cite{tensorflow2015-whitepaper}. The model overall is a sequence to sequence autoencoder, in which a peptide, represented as a sequence, is mapped into a latent distribution which is then sampled to generate a state vector that attempts to reproduce the initial peptide. By setting each amino acid equal to a unit (word) in our input and output sequence, we could think of our model as a translation model with vocabulary size 20 (the number of canonical amino acids). Though this simplistic approach would have worked, we wanted to encorporate advanced translation methods including embedding via negative sampling. This required us to increase our "vocabulary size". Therefore, instead of using individual amino acids as "words" we used amino acid 2-mers, resulting in a vocabulary size of 400 canonical amino acid 2-mers. We replaced non-canonical amino acids with an "unknown" token represented as a '\_' character.

Our data was aquired from uniref90 (which my be downloaded from http://www.uniprot.org/), a dataset that contains proteins and peptides with length of at least 11 amino acids. From this dataset we constrained our model to peptides of less than 100 amino acids resulting in about 6.1 million unique peptides. We uniformly sampled 10000 of these peptides as a test set, and put the rest in our training set. Note that our dataset, while large, is many orders of magnitude less than the $20^{100}$ total possible possible peptides of length 100. By constraining our model to only peptides that are found in nature we heavily bias our model to represent only a small fraction of the combinatorial search space.

Both the training and testing peptides were "bucketed" into 4 buckets based on amino acid 2-mer length. The 2-mer sizes for each bucket were 5-10, 10-20, 20-30 and 30-50. The sequence length distributions are given in Figure \ref{fig:distribution_data_testing} and \ref{fig:distribution_data_training}. Note the scales.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/seq_lengths_test.png}
  \caption{Distribution of amino acid 2-mer sequence lengths in the testing set.}
  \label{fig:distribution_data_testing}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/seq_lengths_train.png}
  \caption{Distribution of amino acid 2-mer sequence lengths in the training set.}
  \label{fig:distribution_data_training}
\end{figure}

Peptides falling in seperate buckets have some bucket-specific model parameters and some some shared model parameters. The bucket-specific parameters allow the model to handle shorter peptides differently from longer peptides. For example, short peptides tend not to start with a methionine, while almost all longer peptides start with a methionine. Shared parameters capture structure that is not specific to bucket length. An example of parameters that are shared are the embedding parameters, which capture the relative simmilarities of amino acid 2-mer. This makes rational sense, because chemically a given 2-mer should behave simillarly whether it is in a long or short peptide, only the distribution of 2-mers is different larger and smaller peptides.

The sequence-to-sequence model starts with a sequence encoder which uses embedding (on 2-mer "word" tokens) and an RNN composed of gated recurrent units (GRU)\cite{chung2014empirical}. The size of the embedding vectors, which is the same as the size of the GRU units, was 256. The RNN had 2 layers, and uses attention\cite{chorowski2014end}. The encoder state was mapped into a vector of length $n_z$ representing the mean of a normal distribution and a vector of length $n_z$ representing the standard deviation of a normal distribution. This disrubution was sampled and decoded using a second (decoder) GRU-based RNN. The type of generative latent distribution model in the middle of our sequence to sequence model is known as a variational autoencoder (VAE) and has been used in other recent molecular representation papers\cite{gomez2016automatic} (though not with a sequence to sequence model). See Figure \ref{model_diagram} for a diagram of the model.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/model_diagram.eps}
  \caption{diagram of sequence to sequence peptide model.}
  \label{model_diagram}
\end{figure}

The objective function used to train our model is a sum of the reconstruction loss, the error between the input and output sequences, and the latent loss, a regularization term describing the complexity of the latent distribution (both measured in log-perplexity). By adding the latent loss together with the reconstruction loss, we prevented over-fitting our model to the training data. The reconstruction loss was defined as the weighted cross entropy for a sequence, a loss function that has worked well in english to french translation, as well as general sequence to sequence applications\cite{vinyals2015grammar}. However, despite these successes, it remains to be seen if it is the most appropriate loss function for peptide sequences (which must consider insertions and deletions).

\section{Results}

- Testing loss and latent loss over time

In order to control the complexity model that captures the peptide distribution, we can train our model with a variety of choices of the hyper paramenter $n_z$. To determine the accuracy of the reconstruction, we chart the loss time on the test set. Figure \ref{fig:loss_fig} shows the reconstruction loss (denoted "loss") and the latent loss (denoted "latent loss") for $n_z$ = 2, 4 and 8.

- Example translations

Another way of showing reconstruction loss is by doing example translations. Here we show 5 different original sequences, each followed by 3 example reconstructions. Note that due to the latent distribution being probabalistic, these example reconstructions are non-deterministic. In fact, due to the use of the RNN encoder and decoders, the reconstructed peptides can be of varying lengths. This allows us to model a form of insertion/deletion mutation.

- Examine embeddings heatmap

Prior to embedding, and after translation, our model uses embedding operations. These operations allow the model to map each 2-mer amino acid token into a dense representation as part of the overall model. We can look at these embeddings and map their relative differences in "embedding space" which captures 2-mer amino acid substitutability.

We hope to in the next few weeks demonstrate the utility of our seq2seq VAE peptide model. We will show a variety of figures including the relative encoding distances between the input tokens (tokens are amino acid 2-mers and encoded prior to being input into the RNN). Inspection of the embeddings will show us whether the token XY is similar to the token YX for various amino acids. Embeddings will also let us see relative substitution rates, which are not explicitly fed into the model from something like a Blossum matrix, but rather are "learned organically".

- Examine "most simmilar peptides in latent space" for 2 peptide examples (mention biology)

Lets take an example of a peptide and find all peptides close to it in latent space.

- Plot distribution of latent space for all peptides in the testing set (and color by bucket?)

Finally, we can plot the latent means of the distributions for each point in the testing set.

\section{Conclusions}

- Model provides a continuous dense representation of peptides

- Model can be used in conjunction with an objective function and labeled data to do semi-supervised learning for drug discovery or for simulating evolution under some "fitness function".

Our model provides more than just a continuous representation of peptides. By controlling the regularization parametter ($n_z$) we can generate a distribution for a given input peptide of peptides that are related to the input peptide. This allows us to do say "guided evolution" where we can draw a mutated peptide from a distribution that models relative substitution rates in the uniref data set. This generated distribution allows us to perform a variety of subsequent machine learning applications on peptides. Say we have an objective function for how well a given peptide performs at binding or for drug treatment. This objective function would be filled with local optima, not differentiable, and expensive to compute (requiring experiment or molecular simulation). We could construct an an optimization strategy of this function more effective than monto carlo or exhastive search, if we use our proposed model as the distribution function in a Metropolis Hastings optimization. Such a proposed application would be similar in theory to previous work, but with a different distribution function\cite{giguere2013improved}.

One large assumption used in this project is the seq2seq error function which serves as the objective function for the whole model. Currently we used a very simple dense sequence comparison (I don't know the details, need to look this up.) but in the future we might want to extend this model by using a seq2seq error function that explicitly accounts for insertions and deletions present in biological sequences. For now, we consider this problem to be mitigated in part by focusing on relatively small peptides (less than 100 amino acids) and we assume that small conserved motifs of amino acids (which is what our model is capturing) are unlikely to contain insertions or deletions. Insertions and deletions are more likely to occur between biologically concerced motifs. Our model currently does allow for insertions and deletions in a general seq2seq way, but it does not handle them explicitly.

\bibliographystyle{model1-num-names}
\bibliography{sample.bib}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.